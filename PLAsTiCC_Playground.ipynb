{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with PLAsTiCC data\n",
    "Author: Kara Ponder (SLAC)\n",
    "\n",
    "This is a playground notebook to understand what needs to be done to get the PLAsTiCC data running. Findings here were pushed to a python script called `use_transformer_plasticc.py` to run fully on the command line. \n",
    "\n",
    "Start by redoing some of the normalizations that were originally done in `astrorapid` package. This gives more control over what the data looks like and how it's saved. Still need to run some pieces of `astrorapid` before this starts. Need the astrorapid from K.Ponder here: https://github.com/kponder/astrorapid. \n",
    "\n",
    "Much of the code for the data is based on the original `astrorapid` code https://github.com/daniel-muthukrishna/astrorapid.\n",
    "\n",
    "After creating new PLAsTiCC data, run the transformer model to check preliminary results. Not meant to be used in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PLAsTiCC training data was generated using `astrorapid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasticc_training_path = 'plasticc_training_data_dir'\n",
    "file_extension = '_minmax_yspline_yt0_multiclass_maybefullci()_zNone_bTrue_ig(88, 92, 65, 16, 53, 6).npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_class=True\n",
    "normalize=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xerr = np.load(plasticc_training_path + 'Xerr' + file_extension, allow_pickle=True)\n",
    "X = np.load(plasticc_training_path + 'X' + file_extension, allow_pickle=True)\n",
    "y = np.load(plasticc_training_path + 'y' + file_extension, allow_pickle=True)\n",
    "\n",
    "timesX = np.load(plasticc_training_path + 'tinterp' + file_extension, allow_pickle=True)\n",
    "labels = np.load(plasticc_training_path + 'labels' + file_extension, allow_pickle=True)\n",
    "\n",
    "#orig_lc = np.load(plasticc_training_path + 'origlc' + file_extension, allow_pickle=True)\n",
    "with open(plasticc_training_path + 'origlc' + file_extension, 'rb') as f:\n",
    "    orig_lc = pickle.load(f)\n",
    "\n",
    "\n",
    "norm_train = np.load(plasticc_training_path + 'normalize_train' + file_extension, allow_pickle=True)\n",
    "norm_test = np.load(plasticc_training_path + 'normalize_test' + file_extension, allow_pickle=True)\n",
    "\n",
    "\n",
    "objids_train = np.load(plasticc_training_path + 'objids_train' + file_extension, allow_pickle=True)\n",
    "objids_test = np.load(plasticc_training_path + 'objids_test' + file_extension, allow_pickle=True)\n",
    "objids_list = np.load(plasticc_training_path + 'objids' + file_extension, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenting the lightcurves only moves the start data around and does not address the non-representitivty in the data.\n",
    "\n",
    "This code is based on the original `astrorapid` code https://github.com/daniel-muthukrishna/astrorapid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_crop_lightcurves(X_local, Xerr_local, y_local, labels_local, timesX_local, orig_lc_local, objids_local):\n",
    "    X_local = copy.copy(X_local)\n",
    "    Xerr_local = copy.copy(Xerr_local)\n",
    "    y_local = copy.copy(y_local)\n",
    "    labels_local = copy.copy(labels_local)\n",
    "    timesX_local = copy.copy(timesX_local)\n",
    "    orig_lc_local = copy.copy(orig_lc_local)\n",
    "    objids_local = copy.copy(objids_local)\n",
    "\n",
    "    newX = np.zeros(X_local.shape)\n",
    "    newXerr = np.zeros(Xerr_local.shape)\n",
    "    newy = np.zeros(y_local.shape)\n",
    "    lenX = len(X_local)\n",
    "    for i in range(lenX):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"new {i} of {lenX}\")\n",
    "        #print(np.shape(timesX_local[i]))\n",
    "        mask = timesX_local[i] >= 0\n",
    "        nmask = sum(mask)\n",
    "        newX[i][:nmask] = X_local[i][mask]\n",
    "        newXerr[i][:nmask] = Xerr_local[i][mask]\n",
    "        if not single_class:\n",
    "            newy[i][:nmask] = y_local[i][mask]\n",
    "        else:\n",
    "            newy[i] = y_local[i]\n",
    "\n",
    "    print(\"Concatenating\")\n",
    "    X_local = np.concatenate((X_local, newX))\n",
    "    Xerr_local = np.concatenate((Xerr_local, newXerr))\n",
    "    y_local = np.concatenate((y_local, newy))\n",
    "    labels_local = np.concatenate((labels_local, labels_local))\n",
    "    timesX_local = np.concatenate((timesX_local, timesX_local))\n",
    "    orig_lc_local = orig_lc_local * 2\n",
    "    objids_local = np.concatenate((objids_local, objids_local))\n",
    "\n",
    "    return X_local, Xerr_local, y_local, labels_local, timesX_local, orig_lc_local, objids_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is based on the original `astrorapid` code https://github.com/daniel-muthukrishna/astrorapid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(labels)))\n",
    "\n",
    "# Count nobjects per class\n",
    "for c in classes:\n",
    "    nobs = len(X[labels == c])\n",
    "    print(c, nobs)\n",
    "\n",
    "# Use class numbers 1,2,3... instead of 1, 3, 13 etc.\n",
    "y_indexes = np.copy(y)\n",
    "for i, c in enumerate(classes):\n",
    "    y_indexes[y == c] = i\n",
    "y = y_indexes\n",
    "\n",
    "if not single_class:\n",
    "    y = y + 1\n",
    "\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Correct shape for keras is (N_objects, N_timesteps, N_passbands) (where N_timesteps is lookback time)\n",
    "X = X.swapaxes(2, 1)\n",
    "Xerr = Xerr.swapaxes(2, 1)\n",
    "\n",
    "print(\"Shuffling\")\n",
    "X, Xerr, y, labels, timesX, orig_lc, objids_list = shuffle(X, Xerr, y, labels, timesX, orig_lc, objids_list)\n",
    "print(\"Done shuffling\")\n",
    "objids_list = np.array(objids_list)\n",
    "\n",
    "train_idxes = [i for i, objid in enumerate(objids_list) if objid in objids_train]\n",
    "test_idxes =  [i for i, objid in enumerate(objids_list) if objid in objids_test]\n",
    "X_train = X[train_idxes]\n",
    "Xerr_train = Xerr[train_idxes]\n",
    "y_train = y[train_idxes]\n",
    "labels_train = labels[train_idxes]\n",
    "timesX_train = timesX[train_idxes]\n",
    "orig_lc_train = [orig_lc[i] for i in train_idxes]\n",
    "objids_train = objids_list[train_idxes]\n",
    "X_test = X[test_idxes]\n",
    "Xerr_test = Xerr[test_idxes]\n",
    "y_test = y[test_idxes]\n",
    "labels_test = labels[test_idxes]\n",
    "timesX_test = timesX[test_idxes]\n",
    "orig_lc_test = [orig_lc[i] for i in test_idxes]\n",
    "objids_test = objids_list[test_idxes]\n",
    "\n",
    "\n",
    "X_train, Xerr_train, y_train, labels_train, timesX_train, orig_lc_train, objids_train = augment_crop_lightcurves(X_train, Xerr_train, y_train, labels_train, timesX_train, orig_lc_train, objids_train)\n",
    "\n",
    "X_train, Xerr_train, y_train, labels_train, timesX_train, orig_lc_train, objids_train = shuffle(X_train, Xerr_train, y_train, labels_train, timesX_train, orig_lc_train, objids_train)\n",
    "\n",
    "\n",
    "# Sample weights\n",
    "counts = np.unique(labels_train, return_counts=True)[-1]\n",
    "class_weights = max(counts) / counts\n",
    "class_weights = dict(zip(range(len(counts)), class_weights))\n",
    "print(\"Class weights:\", class_weights)\n",
    "l_train_indexes = np.copy(labels_train)\n",
    "for i, c in enumerate(classes):\n",
    "    l_train_indexes[l_train_indexes == c] = i\n",
    "sample_weights = np.zeros(len(l_train_indexes))\n",
    "for key, val in class_weights.items():\n",
    "    sample_weights[l_train_indexes == key] = val\n",
    "\n",
    "# #NORMALISE\n",
    "if normalize:\n",
    "    X_train = X_train.copy()\n",
    "    Xerr_train = Xerr_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "    Xerr_test = Xerr_test.copy()\n",
    "\n",
    "    def do_normalization(d, derr):\n",
    "        lc_norm = [[min(d[i, :, :].flatten()), max(d[i, :, :].flatten())] for i in range(len(d))]\n",
    "\n",
    "        for i in range(len(d)):\n",
    "            wh = np.where((d[i, :, :] > 0.) | (d[i, :, :] < 0.))\n",
    "            d[i, :, :][wh] =  (d[i, :, :][wh] - lc_norm[i][0]) / (lc_norm[i][1] - lc_norm[i][0])\n",
    "            derr[i, :, :][wh] =  (derr[i, :, :][wh] - lc_norm[i][0]) / (lc_norm[i][1] - lc_norm[i][0])\n",
    "        return d, derr, lc_norm\n",
    "\n",
    "    X_train, Xerr_train, lc_norm_train = do_normalization(X_train, Xerr_train)\n",
    "    X_test, Xerr_test, lc_norm_test = do_normalization(X_test, Xerr_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weight map\n",
    "X_wgtmap_train = np.zeros(np.shape(Xerr_train))\n",
    "X_wgtmap_validation = np.zeros(np.shape(Xerr_test))\n",
    "\n",
    "X_wgtmap_train[np.where(Xerr_train != 0)] = 1.0/Xerr_train[np.where(Xerr_train != 0)]**2 #np.ones(X_train.shape)\n",
    "X_wgtmap_validation[np.where(Xerr_test != 0)] = 1.0/Xerr_test[np.where(Xerr_test != 0)]**2 #np.ones(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this data so you can use it next time and not rerun these sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_dir = 'your_scratch_location'\n",
    "file_extension = '_minmax_yspline_yt0_multiclass_maybefullci()_zNone_bTrue_ig(88,92,65,16,53,6).npy'\n",
    "\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"X_train\" + file_extension), X_train)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"X_wgtmap_train\" + file_extension), X_wgtmap_train)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"lc_norm_train\" + file_extension), lc_norm_train)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"y_train\" + file_extension), y_train)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"labels_train\" + file_extension), labels_train)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"timesX_train\" + file_extension), timesX_train)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"orig_lc_train\" + file_extension), orig_lc_train)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"objids_train\" + file_extension), objids_train)\n",
    "\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"X_valid\" + file_extension), X_test)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"X_wgtmap_valid\" + file_extension), X_wgtmap_validation)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"lc_norm_valid\" + file_extension), lc_norm_test)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"y_valid\" + file_extension), y_test)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"labels_valid\" + file_extension), labels_test)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"timesX_valid\" + file_extension), timesX_test)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"orig_lc_valid\" + file_extension), orig_lc_test)\n",
    "np.save(os.path.join(training_set_dir,\n",
    "                    \"objids_valid\" + file_extension), objids_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading in the data, be sure to run the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the data set we just generated, run below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "lc_data = X_train\n",
    "\n",
    "pre_mask_map = np.ma.masked_values(X_train, 0)\n",
    "mask_map = np.ones(np.shape(X_train))\n",
    "mask_map[pre_mask_map.mask] = 0.0\n",
    "\n",
    "pre_mask_map_validation = np.ma.masked_values(X_test, 0)\n",
    "mask_map_validation = np.ones(np.shape(X_test))\n",
    "mask_map_validation[pre_mask_map_validation.mask] = 0.0\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((lc_data, mask_map, X_wgtmap_train))\n",
    "\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, mask_map_validation, X_wgtmap_validation))\n",
    "\n",
    "batch_ds = dataset.batch(batch_size)\n",
    "batch_ds_valid = dataset_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use a previously generated data set, use this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'your/training/data/location'\n",
    "data_extension = '_nobs50_timestep3_singleclass_noaug_train60_minmax_nospline_v02ci()_zNone_bTrue_ig(88,92,65,16,53,6).npy'\n",
    "\n",
    "\n",
    "# Read in dataset\n",
    "_lc_data = np.load(data_dir + 'X_train' + data_extension)\n",
    "_wgt_map = np.load(data_dir + 'X_wgtmap_train' + data_extension)\n",
    "_pre_mask_map = np.ma.masked_values(_lc_data, 0)\n",
    "_mask_map = np.ones(np.shape(_lc_data))\n",
    "_mask_map[_pre_mask_map.mask] = 0.0\n",
    "\n",
    "_dataset = tf.data.Dataset.from_tensor_slices((_lc_data, _mask_map, _wgt_map))\n",
    "\n",
    "_lc_data_valid = np.load(data_dir + 'X_valid' + data_extension)\n",
    "_wgt_map_valid = np.load(data_dir + 'X_wgtmap_valid' + data_extension)\n",
    "\n",
    "_pre_mask_map_valid = np.ma.masked_values(_lc_data_valid, 0)\n",
    "_mask_map_valid = np.ones(np.shape(_lc_data_valid))\n",
    "_mask_map_valid[_pre_mask_map_valid.mask] = 0.0\n",
    "\n",
    "_dataset_valid = tf.data.Dataset.from_tensor_slices((_lc_data_valid, _mask_map_valid, _wgt_map_valid))\n",
    "\n",
    "_batch_ds = _dataset.batch(batch_size)\n",
    "_batch_ds_valid = _dataset_valid.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use the test data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lc_data = np.load('lc_data.npy')\n",
    "\n",
    "_wgt_map = np.load('weightmap.npy')\n",
    "\n",
    "_mask_map = np.ones(np.shape(_wgt_map))\n",
    "_mask_map[np.where(_wgt_map == 0)] = 0\n",
    "\n",
    "_dataset = tf.data.Dataset.from_tensor_slices((_lc_data, _mask_map, _wgt_map))\n",
    "_batch_ds = _dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer as tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "d_model = 128 # input vector must have length d_model\n",
    "target_vocab_size = 6  # possible results to choose from\n",
    "\n",
    "lc_length = 100 +1 #50 +1 # light curve length\n",
    "input_vocab_size = lc_length\n",
    "\n",
    "## hyperparameters:\n",
    "num_layers = 6\n",
    "dropout_rate = 0\n",
    "dff = 64 # hidden layer size of the feed forward network, needs to be larger than 24, factor of 2^x\n",
    "num_heads = 8 # d_model % num_heads == 0\n",
    "\n",
    "kld_alpha = 0.4 \n",
    "kld_rho = 1e-10\n",
    "\n",
    "# LC stuff\n",
    "N_days = 50 + 1\n",
    "Nf = 6 # number of filters\n",
    "\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with a custom step scheduler. Used in the original Transformer paper (_Attention is all you need_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=0):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(tf.multiply(self.d_model, 100000.0)) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model, warmup_steps=10)\n",
    "plt.plot(learning_rate(tf.range(100, dtype=tf.float32)))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define possible loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE(tf.keras.losses.Loss):\n",
    "    def __init__(self, name=\"rmse\"):\n",
    "        super().__init__(name=name)#\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "        return tf.math.sqrt(mse)\n",
    "\n",
    "def loss_kld(layer1, layer2, alpha=kld_alpha):\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    layer1 = layer1[0]\n",
    "    layer1 = tf.math.abs(layer1)\n",
    "    layer2 = layer2[0]\n",
    "    layer2 = tf.math.abs(layer2)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        ones = tf.ones(layer1.shape, dtype=tf.float32)\n",
    "        rhoc = kld_rho\n",
    "        rho = rhoc*ones\n",
    "\n",
    "        def kld(layer):\n",
    "            kld_1 = tf.math.multiply(rhoc, tf.math.log(tf.math.divide_no_nan(rho, layer)))\n",
    "            kld_2 = tf.math.multiply((1.0 - rhoc), tf.math.divide_no_nan(tf.math.log(ones-rho), tf.math.log(ones-layer)))\n",
    "            return tf.reduce_sum(kld_1 + kld_2) #kld_1_without_nans + kld_2_without_nans)\n",
    "\n",
    "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "        rmse = tf.math.sqrt(mse)\n",
    "        return rmse + tf.multiply(alpha, (kld(layer1) + kld(layer2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "encoder = tran.Encoder(num_layers, d_model, num_heads, dff,\n",
    "                       lc_length, dropout_rate, embed=True)\n",
    "\n",
    "decoder = tran.Decoder(num_layers, d_model, num_heads, dff,\n",
    "                       lc_length, dropout_rate, embed=True)\n",
    "\n",
    "final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "\n",
    "inp = tf.keras.layers.Input(shape=(None,Nf))\n",
    "target = tf.keras.layers.Input(shape=(None,Nf))\n",
    "maskmap = tf.keras.layers.Input(shape=(None,Nf))\n",
    "\n",
    "\n",
    "x = encoder(inp)\n",
    "x = decoder(target, x, mask=tran.create_decoder_masks(inp, target))\n",
    "x = final_layer(x)\n",
    "\n",
    "\n",
    "mx = tf.keras.layers.Multiply()([x, maskmap])\n",
    "model = tf.keras.models.Model(inputs=[inp, target, maskmap], outputs=mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the loss function and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = {'MSE': tf.keras.losses.MeanSquaredError(),\n",
    "             'MSLE': tf.keras.losses.MeanSquaredLogarithmicError(),\n",
    "             'Huber': tf.keras.losses.Huber(),\n",
    "             'MAE': tf.keras.losses.MeanAbsoluteError(),\n",
    "             'LCE': tf.keras.losses.LogCosh(),\n",
    "             'RMSE': RMSE(),\n",
    "             'KLD_RMSE':loss_kld(model.get_layer(name='encoder').get_weights(),\n",
    "                                 model.get_layer(name='decoder').get_weights()),\n",
    "             }\n",
    "\n",
    "loss_object = loss_dict['KLD_RMSE']\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "# Compile and run the model\n",
    "model.compile(optimizer=optimizer, loss=loss_function, \n",
    "             metrics=[tf.keras.metrics.MeanSquaredError(),\n",
    "                           tf.keras.metrics.MeanAbsoluteError(),\n",
    "                           tf.keras.metrics.RootMeanSquaredError(),\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 0\n",
    "for (batch, _) in enumerate(_batch_ds):\n",
    "    num_batches = batch\n",
    "\n",
    "num_batches_valid = 0\n",
    "for (batch, _) in enumerate(_batch_ds_valid):\n",
    "    num_batches_valid = batch\n",
    "\n",
    "    \n",
    "# Set up to run the fit\n",
    "def generator(data_set):\n",
    "    while True:\n",
    "        for in_batch, mask_batch, wgt_batch in data_set: \n",
    "            yield ( [in_batch , in_batch[:, :-1, :],  mask_batch[:, 1:, :], wgt_batch] , in_batch[:, 1:, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = generator(_batch_ds),\n",
    "                    validation_data = generator(_batch_ds_valid),\n",
    "                    epochs=50, #EPOCHS,\n",
    "                    steps_per_epoch=num_batches,\n",
    "                    validation_steps = num_batches_valid,\n",
    "                    #verbose=0,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the weights or load in previous weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('your/transformer/weights/transformer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('your/transformer/weights/transformer.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using one of the training data to see if it can at least do that. (last time I checked it could not do it too well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 100\n",
    "decoder_input=[[0.0]*Nf]## or 0\n",
    "output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "for i in range(N_days-1):\n",
    "    predictions = model([_lc_data[loc][tf.newaxis, :, :], output, _mask_map[loc][tf.newaxis, 1:i+1, :]]) #.predict\n",
    "    \n",
    "    predictions = predictions[: ,-1:, :] ## CHECKKK\n",
    "    \n",
    "    output = tf.concat([output, predictions], axis=1)\n",
    "    tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(_lc_data[loc][1:, 0], 's', ls = '-.', color='tab:blue')\n",
    "plt.plot(_lc_data[loc][1:, 2], 's', ls = '-.', color='tab:orange')\n",
    "plt.plot(_lc_data[loc][1:, 3], 's', ls = '-.', color='tab:red')\n",
    "\n",
    "plt.plot(output[0][1:, 2], 'o', lw=2, color='tab:orange')\n",
    "plt.plot(output[0][1:, 0], 'o', lw=2, color='tab:blue', label='predicted lc') # this might finally be right!\n",
    "plt.plot(output[0][1:, 1], 'o', lw=2)\n",
    "plt.plot(output[0][1:, 3], 'o', lw=2, color='tab:red')\n",
    "plt.plot(output[0][1:, 4], 'o', lw=2)\n",
    "plt.plot(output[0][1:, 5], 'o', lw=2)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('brightness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
