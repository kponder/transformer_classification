{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine model fit using test data\n",
    "\n",
    "Author: Kara Ponder (SLAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import transformer as tran\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "d_model = 128 #6 # input vector must have length d_model\n",
    "target_vocab_size = 6 #100 #4  # possible results to choose from\n",
    "\n",
    "lc_length = 100 +1 # light curve length\n",
    "input_vocab_size = lc_length\n",
    "\n",
    "## hyperparameters:\n",
    "num_layers = 8 # 4 #\n",
    "dropout_rate = 0.0\n",
    "dff = 64 # hidden layer size of the feed forward network, needs to be larger than 24\n",
    "num_heads = 8 #6 #3 # d_model % num_heads == 0\n",
    "\n",
    "# LC stuff\n",
    "N = 10000 # number of objects\n",
    "N_days = 100 + 1\n",
    "Nf = 6 # number of filters\n",
    "num_classes = 4\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.loadtxt(YOUR_LOSS_TEXT_FILE)\n",
    "\n",
    "plt.plot(loss, label='WHAT MAKES ME SPECIAL')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at predictions\n",
    "\n",
    "Define the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.00001)\n",
    "\n",
    "def loss_kld(layer1, layer2, alpha=0.3):\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    layer1 = layer1[0]\n",
    "    layer1 = tf.math.abs(layer1)\n",
    "    layer2 = layer2[0]\n",
    "    layer2 = tf.math.abs(layer2)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        ones = tf.ones(layer1.shape, dtype=tf.float32)\n",
    "        rhoc = 0.00001\n",
    "        rho = rhoc*ones\n",
    "        \n",
    "\n",
    "        def kld(layer):\n",
    "            kld_1 = tf.math.multiply(rhoc, tf.math.log(tf.math.divide_no_nan(rho, layer)))\n",
    "            kld_2 = tf.math.multiply((1.0 - rhoc), tf.math.divide_no_nan(tf.math.log(ones-rho), tf.math.log(ones-layer)))\n",
    "            return tf.reduce_sum(kld_1 + kld_2) #kld_1_without_nans + kld_2_without_nans)\n",
    "\n",
    "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "        rmse = tf.math.sqrt(mse)\n",
    "        return rmse + tf.multiply(alpha, (kld(layer1) + kld(layer2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tran.Encoder(num_layers, d_model, num_heads, dff,\n",
    "                       lc_length, dropout_rate, embed=True)\n",
    "\n",
    "decoder = tran.Decoder(num_layers, d_model, num_heads, dff,\n",
    "                       lc_length, dropout_rate, embed=True)\n",
    "\n",
    "final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "inp = tf.keras.layers.Input(shape=(None,Nf))\n",
    "target = tf.keras.layers.Input(shape=(None,Nf))\n",
    "wgts = tf.keras.layers.Input(shape=(None,Nf))\n",
    "mask = tf.keras.layers.Input(shape=(None,Nf))\n",
    "\n",
    "x = tf.keras.layers.Masking(mask_value=0.)(inp)\n",
    "x = encoder(x)\n",
    "x = decoder(target, x, mask=tran.create_decoder_masks(inp, target))\n",
    "x = final_layer(x)\n",
    "mx = tf.keras.layers.Multiply()([x, mask])\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[inp, target, mask], outputs=mx)\n",
    "model.compile(optimizer=optimizer, loss=loss_kld(model.get_layer(name='encoder').get_weights(),\n",
    "                                                 model.get_layer(name='decoder').get_weights()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('LOCATION_OF_SAVED_WEIGHTS.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(lc_data, mask_map, wgt_map):\n",
    "    inp_lc = tf.expand_dims(lc_data, 0)\n",
    "    inp_lc_wgt = tf.expand_dims(wgt_map, 0)\n",
    "    inp_lc_mask = tf.expand_dims(mask_map, 0)\n",
    "    decoder_input=tf.constant([[0.0]*Nf]) # This depends on what the baseline values are. Typically zero #\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    for i in range(N_days-1):\n",
    "        predictions = model([inp_lc, output,  inp_lc_mask[:, 1:i+2, :], inp_lc_wgt])#, training=False) # if batching may need predict.\n",
    "        predictions = predictions[: ,-1:, :] #* tf.expand_dims(inp_lc_wgt[:, i,:],0)\n",
    "        output = tf.concat([output, predictions], axis=1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in some data. Some fake data is included in the repo but it is not the PLAsTiCC data. It is simplified data based on an empirical function of the rise and fall of Type Ia Supernovae called the Bazin function/parameterization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_data = np.load('lc_data.npy')\n",
    "real_lc_data = np.load('real_lc_data.npy')\n",
    "\n",
    "wgt_map = np.load('weightmap.npy')\n",
    "\n",
    "mask_map = wgt_map * 1/0.1**2\n",
    "mask_map[np.where(mask_map == 0)] = 1/2**2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((lc_data, real_lc_data, wgt_map, mask_map))\n",
    "batch_ds = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using test data and the evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_lc = tf.constant(lc_data[0])\n",
    "check_lc_mask = tf.constant(wgt_map[0], dtype=tf.float32)\n",
    "check_lc_wgt = tf.constant(mask_map[0], dtype=tf.float32)\n",
    "pred=evaluate(check_lc, check_lc_mask, check_lc_wgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lc_data[0, 1:, 0], 'ro', lw = 2, alpha=0.4, label='lc data')\n",
    "plt.plot(pred[1:, 0], 'o', lw=2, label='predicted lc ', alpha=0.5)\n",
    "plt.plot(real_lc_data[0, 1:, 0], 'ko', lw = 2, alpha=0.4, label='model')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_tf",
   "language": "python",
   "name": "new_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
